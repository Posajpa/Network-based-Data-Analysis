---
title: "Untitled"
author: "Seyed Pooria Sajadi Parsa"
date: "2024-04-06"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Data Preparation

## Installing and loading packages


```{r}
# Installing necessary packages
# install.packages("BiocManager")
# BiocManager::install("GEOquery")
# BiocManager::install("useful")
# BiocManager::install("randomForest")
# BiocManager::install("genefilter")
# BiocManager::install("GEOquery")
# BiocManager::install("pROC")
# install.packages("caret")
# install.packages("igraph")
# BiocManager::install("rScudo")
#install.packages("gprofiler2")
# install.packages("BiocManager")
# BiocManager::install("KEGGREST")
# BiocManager::install("KEGGgraph")
# BiocManager::install("AnnotationDbi")
# BiocManager::install("org.Hs.eg.db")
# install.packages("pathfindR")
# install.packages("xfun")





# Loading necessary libraries
library("GEOquery")
library("useful")
library(randomForest)
library("genefilter")
library("MASS")
library("pROC")
library("caret")
library(dplyr)
library("rScudo")
library("igraph")
library(gprofiler2)
library("KEGGgraph")
library("AnnotationDbi")
library("org.Hs.eg.db")
library("pathfindR")

```

## setting the seed for reproducibility

```{r}
set.seed(1112)
```

## Loading the dataset

```{r}
# Download and access the processed data from GEO
gse <- getGEO('GSE168198') 

# Select the first dataset
gse <- gse[[1]]

# View a summary of the dataset
show(gse)

```

## Exploring Expression Values

```{r}
# Extract the expression matrix from the dataset
ex <- exprs(gse)

# View the first few rows of the expression matrix
head(ex)

# Check the total number of genes and samples
cat("Number of genes:", nrow(ex), "\n")
cat("Number of samples:", ncol(ex), "\n")

# Examine the dimensions of the expression matrix
dim(ex)
```

## Analyzing Value Distributions

```{r}
# Create a boxplot to visualize the distribution of expression values
boxplot(ex)
```

As we can see in the plot, the distribution of gene expression values is skewed so we'll need to perform a log2 transformation on the expression values.

```{r}
# Apply log2 transformation to the expression matrix
ex2 <- log2(ex)

# Remove missing values (NAs) after transformation
ex2 <- na.omit(as.matrix(ex2)) 

# Create a boxplot to visualize the distribution of log-transformed expression values
boxplot(ex2, las = 2)

# Create a factor variable for sample classification
group <- factor(c(rep('control', 8), rep('cancer', 8))) 
```

# Principal Component Analysis (PCA)

PCA helps reduce the dimensionality of the data while capturing most of the variance.
This is useful for visualization purposes in high-dimensional datasets such as gene data.

First, we perform PCA on the data and generate a plot to visualize the variance captured by each principal component.

```{r}
# Perform PCA on the transposed expression matrix
pca <- prcomp(t(ex2))

# View summary statistics of the PCA analysis
summary(pca)

# Visualize the variance captured by each principal component
screeplot(pca)
```

Next, We'll  
 
```{r}
# Define colors for sample groups (control: blue, cancer: red)
grpcol <- c(rep("blue", 8), rep("red", 8))

# Draw PCA plot for the first two principal components
plot(
  pca$x[, 1],
  pca$x[, 2],
  xlab = "PCA1",
  ylab = "PCA2",
  main = "PCA for components 1&2",
  type = "p", pch = 10, col = grpcol
)
# text(pca$x[, 1], pca$x[, 2], rownames(pca$x), cex = 0.75)
```

As we can see in the plot, there is some degree of separation between the red (cancer) and blue (control) points. This suggests that there are differences in gene expression between the control and cancer samples that can be captured by the first two principal components.
However, there is also some overlap between the clusters. This indicates that while there are differences, the distinction between control and cancer samples is not entirely clear-cut when reduced to these two components.

# Clustering

## K-means

K-means is a clustering algorithm that partitions data points into a predefined number of groups (clusters) based on their similarity. The goal is to minimize the within-cluster sum of squares (variance) for each cluster.

```{r}
# Set the number of clusters (k)
k <- 2

# Perform K-means clustering
kmeans_result <- kmeans(t(ex2), k)

# Show how data points are distributed across clusters
cat("Cluster distribution:\n")
table(kmeans_result$cluster)

# Visualization using PCA for dimensionality reduction
plot(kmeans_result, data = t(ex2)) +
  geom_text(aes(label = group), hjust = 0, vjust = 0)  # Label points by group

# geom_text(aes(label = gse1@phenoData@data[["gender:ch1"]]),
#           hjust = 0,
#           vjust = 0)
# geom_text(aes(label = gse1@phenoData@data[["characteristics_ch1.1"]]),
#           hjust = 0,
#           vjust = 0)
 
```

Based on the results The K-means clustering with two pre-defined groups (k=2) identified potential groupings within the gene expression data. The visualization suggests that these groups might correspond to underlying biological differences between the samples.
However, the presence of overlapping data points and misclassifications indicates that the separation between the clusters is not perfect.

## Hierarchical Clustering (Hclust)

Hierarchical clustering is an unsupervised learning technique that groups data points into a hierarchy based on their similarity. Unlike K-means, which requires pre-defining the number of clusters (k), Hclust builds a hierarchy and allows us to choose the number of clusters based on the desired level of granularity.

```{r}
# Calculate pairwise distances between samples based on gene expression values
dist_matrix <- dist(t(ex2))

# Perform hierarchical clustering using complete linkage method
hc_result <- hclust(dist_matrix, method = "complete")

# Define the number of clusters (k)
k <- 2

# Assign samples to clusters
groups <- cutree(hc_result, k = k)

# View the distribution of samples across the clusters
table(groups)

# View the distribution of samples across the clusters
plot(hc_result, hang <- -1, labels = group)

# Highlight the chosen clusters with red boxes
rect.hclust(
  hc_result,
  k,
  which = NULL,
  x = NULL,
  h = NULL,
  border = 2,
  cluster = NULL
)
```

As we can see in the plot, the hierarchical clustering reveals two main groups (highlighted by red boxes) based on the gene expression data. However, unlike ideal clustering, these groups contain a mix of cancer and control samples, indicating that the separation between the classes is not perfect.
Moreover, the number of samples in each group is not balanced. This imbalance can further hinder the ability of hierarchical clustering to achieve a clear distinction between control and cancer samples

# RANDOM FOREST (RF)

We now use of Random Forest to identify genes that hold the key to differentiating control and cancer samples. By analyzing the importance scores assigned by a Random Forest model, we can prioritize genes that play a crucial role in distinguishing these two groups.

```{r}
# Build the Random Forest model
rf <- randomForest(x = t(ex2),
                   y = as.factor(group),
                   ntree = 1000)
rf
```

We can see that The high OOB error rate and the confusion matrix both suggest the Random Forest model might not be optimal for differentiating cancer and control samples based on the current settings. The model seems to have a significant bias towards misclassifying cancer samples and has a high overall error rate.

```{r}
# Ranking Genes by importance
plot(sort(rf$importance, decreasing = TRUE))

# # Extract the most important genes
# imp.temp <- abs(rf$importance[, ])
# top200 <- order(rf$importance, decreasing = TRUE)
```

# Linear Discriminant Analysis (LDA)

LDA is a statistical method specifically designed to identify discriminatory variables that can effectively separate groups, particularly useful when dealing with two classes (cancer vs. control in our case).

First, we prepare the data.

```{r}
# Perform t-tests on each gene (row) to compare control vs. cancer groups
tt <- rowttests(ex2, group)

# Select genes with statistically significant differences (p-value < 0.1)
keepers <- which(tt$p.value < 0.1)

# Create new expression matrix with only informative genes
ex3 <- ex2[keepers, ]
tex3 <- t(ex3)
dat <- cbind(as.data.frame(tex3), group)
colnames(dat)[ncol(dat)] <- "TYPE"

# Split data into Training and Testing Sets
n.controls <- 8
n.affected <- 8
train <- sample(1:(n.controls), (n.controls - 4))
test <- setdiff(1:(n.controls), train)
test <- c(test, test + 8)
train <- c(train, train + 8)
```

Now, we build and Visualize the LDA Model.

```{r}
# Build the LDA model
mod <- lda(TYPE ~ ., data=dat, prior = c(0.5,0.5),
subset = train)

# Visualize the separation between control and cancer samples based on LDA
plot(mod)
```

```{r}
# Make predictions on the training data for further analysis
mod.values <- predict(mod, dat[train,])

# Plot the first LDA axis and color points by their actual class labels
plot(mod.values$x[,1], ylab=c("LDA Axis"))
text(mod.values$x[,1], col=c(as.numeric(dat[train,"TYPE"])+10))
```

Looking at the plots, we can observe how the LDA model separates the control and cancer samples in the training data set.
Ideally, the training set plot should show clear separation between the control and cancer clusters, indicating that the model has learned effective boundaries to distinguish the groups, However, we can see that there is a misclassification for one of the samples.

Now we evaluate the Model Performance on Test Data.

```{r}
# Make predictions on the unseen test data set
preds <- predict(mod, dat[test, ])

# Create a confusion matrix
table(as.numeric(preds$class), as.numeric(dat[test, "TYPE"]))
```

Based on the confusion matrix, the LDA model seems to perform well in identifying cancer samples in the test set. There were no missed detections, which is crucial for cancer diagnosis.

```{r}
# Generate the ROC curve for the LDA model on the test set
roc_lda <- plot.roc(as.numeric(preds$class), as.numeric(dat[test, "TYPE"]))
```

The ROC curve demonstrates a good performance of the LDA model in classifying cancer samples. The curve leans towards the top-left corner of the ROC space, indicating a high true positive rate (TPR) for cancer detection at a low false positive rate (FPR).

# LDA, RF & Lasso using CARET

In this part we compare the performance of three classification models (Linear Discriminant Analysis (LDA), Random Forest (RF), and Lasso) using the caret package.

## Setting Up Cross-Validation

```{r}
# Define cross-validation settings
control <- trainControl(
  method = "repeatedcv",
  number = 4,
  repeats = 10,
  savePredictions = TRUE
)
metric <- "Accuracy"
```

## LDA Model

```{r}
# Train the LDA model
fit.lda <- train(
  TYPE ~ .,
  data = dat,
  method = "lda",
  metric = metric,
  trControl = control
)
```

## RF Model

```{r}
# Train the Random Forest model using the same structure
fit.rf <- train(
  TYPE ~ .,
  data = dat,
  method = "rf",
  metric = metric,
  trControl = control
)
```

## Lasso Model

```{r}
# Train the Lasso model using glmnet for classification
fit.lasso <- train(
  TYPE ~ .,
  data = dat,
  method = "glmnet",
  family = "binomial",
  trControl = control,
  metric = metric
)
```

Now that we have trained all three models, we compare their performances.

## Comparing Model Performances

```{r}
# Combine results from all models
results <- resamples(list(RF = fit.rf, LDA = fit.lda, Lasso = fit.lasso), skip = TRUE)

# Print a summary table of performance metrics for each model
summary(results)
```

Based on the summary, we can see that both RF and LDA achieved a very high mean accuracy (around 0.99) with a minimum of 0.75 and a maximum of 1. This suggests that they were very successful in correctly classifying samples and appear to be strong candidates for classifying.

```{r}
# Create a visualization to compare model accuracy
ggplot(results) + labs(y = "Accuracy")
```

The plot also confirms the observations from the summary table. The boxplots for both Random Forest (RF) and Linear Discriminant Analysis (LDA) are clustered towards the right side of the plot, with most of the boxes and whiskers falling above 0.95 accuracy. This indicates that they generally achieved high accuracy across the resamples.
The Lasso model's boxplot is significantly lower than the others. The entire box is well below 1.0, and the upper whisker only reaches around 0.9. This suggests that Lasso had considerably lower accuracy compared to RF and LDA.

Comparing RF to LDA we can see that, while LDA also achieved high accuracy, the slight edge goes to RF due to its lower variability, implying consistent accuracy across different data splits in the cross-validation. This makes it a reliable choice as the main model we want to use.

# Extracting Top Genes From The Best Model

Now that we've identified Random Forest (RF) as the best performing model for our classification task, let's extract the most important genes based on their contribution to the model's predictions. These genes are likely to be the most informative features for distinguishing between the classes in our data.

## Identifying Most Important Genes
```{r}
# access the final Random Forest model and extract probe names
probe.names <- rownames(fit.rf[["finalModel"]][["importance"]])
probe.names <- gsub("`", "", probe.names)

# order genes by importance (descending)
gene_list1 <- probe.names[order(fit.rf[["finalModel"]][["importance"]], decreasing =
                                  TRUE)]
```

## Mapping Probe IDs to Gene Names

```{r}
mapping_df <- read.table(
  "GPL29809_family.soft",
  sep = "\t",
  header = TRUE,
  skip = 59,
  nrows = 46066
)
filtered_df <- mapping_df %>% filter(ID %in% gene_list1)
gene_list2 <- filtered_df$ORF
```

## Selecting Top K Genes and Writing Output:

```{r}
# Set the number (K) of top genes to select
k <- 300

# Write top genes to a text file
write.table(gene_list2[1:k], file = paste0("top",k,".txt"), quote=FALSE, row.names = FALSE, col.names=FALSE)
```

# RSCUDO

```{r}
inTrain <- createDataPartition(group, list = FALSE)
trainData <- ex3[, inTrain]
testData <- ex3[, -inTrain]
```

```{r}
# analyze training set
trainRes <- scudoTrain(trainData, groups = group[inTrain], nTop = 25, nBottom = 25, alpha = 0.05)
trainRes
```

```{r}
# generate and plot map of training samples
trainNet <- scudoNetwork(trainRes, N = 0.4)
scudoPlot(trainNet, vertex.label = NA)
```

```{r}
# perform validation using testing samples
testRes <- scudoTest(trainRes, testData, group[-inTrain],
nTop = 25, nBottom = 25)
testNet <- scudoNetwork(testRes, N = 0.4)
scudoPlot(testNet, vertex.label = NA)
```

```{r}
# identify clusters on map
testClust <- cluster_spinglass(testNet, spins = 2)
plot(testClust, testNet, vertex.label = NA)
```

```{r}
# perform classification
classRes <- scudoClassify(trainData, testData, N = 0.4, nTop = 12, nBottom = 12,
trainGroups = group[inTrain], alpha = 0.5)
confusionMatrix(classRes$predicted, group[-inTrain])
```

# Pathfinder

First we create a the dataframe required

```{r}
# tt <- rowttests(ex2, group)
pathfinder_df <- data.frame(Gene.Symbol = mapping_df$ORF, Log2.FoldChange = tt$dm, p.values = tt$p.value)
```

```{r}

# pathway enrichment
RA <- run_pathfindR(pathfinder_df, iterations = 1, gene_sets = 'KEGG')
head(RA)

# clutser enriched terms
RA_clu <- cluster_enriched_terms(RA)

# term-gene graph of top 10 terms
term_gene_graph(RA)

```
